{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "LLM Evaluation Patterns",
  "description": "Patterns for evaluating LLMs, RAG systems, and AI applications",
  "version": "1.0.0",
  "axiomAlignment": {
    "A1_verifiability": "Systematic evaluation enables verification of AI behavior",
    "A3_transparency": "Metrics and benchmarks make performance explicit"
  },
  "evaluation_types": {
    "offline_evaluation": {
      "description": "Evaluate on static test sets before deployment",
      "when_to_use": "Development, model comparison, before release"
    },
    "online_evaluation": {
      "description": "Monitor performance on live traffic",
      "when_to_use": "Production monitoring, A/B testing"
    },
    "llm_as_judge": {
      "description": "Use LLM to evaluate other LLM outputs",
      "when_to_use": "Complex quality assessment, scalable evaluation"
    }
  },
  "rag_evaluation": {
    "ragas": {
      "description": "Standard RAG evaluation framework",
      "code_example": "from ragas import evaluate\nfrom ragas.metrics import (\n    faithfulness,\n    answer_relevancy,\n    context_precision,\n    context_recall,\n    context_utilization\n)\nfrom datasets import Dataset\n\n# Prepare evaluation data\neval_data = {\n    'question': questions,  # List of questions\n    'answer': answers,  # Generated answers\n    'contexts': contexts,  # Retrieved contexts for each question\n    'ground_truth': ground_truths  # Expected answers\n}\n\ndataset = Dataset.from_dict(eval_data)\n\n# Run evaluation\nresults = evaluate(\n    dataset,\n    metrics=[\n        faithfulness,  # Is answer grounded in context?\n        answer_relevancy,  # Is answer relevant to question?\n        context_precision,  # Are retrieved contexts relevant?\n        context_recall  # Did we get all needed context?\n    ]\n)\n\nprint(f'Faithfulness: {results[\"faithfulness\"]:.3f}')\nprint(f'Answer Relevancy: {results[\"answer_relevancy\"]:.3f}')\nprint(f'Context Precision: {results[\"context_precision\"]:.3f}')\nprint(f'Context Recall: {results[\"context_recall\"]:.3f}')",
      "metrics": {
        "faithfulness": "Measures if answer is grounded in retrieved context (0-1)",
        "answer_relevancy": "Measures if answer addresses the question (0-1)",
        "context_precision": "Measures precision of retrieved contexts (0-1)",
        "context_recall": "Measures if all relevant info was retrieved (0-1)",
        "context_utilization": "Measures how well context was used in answer"
      },
      "best_practices": [
        "Evaluate on representative questions",
        "Include edge cases in test set",
        "Track metrics over time",
        "Set threshold for each metric"
      ]
    },
    "retrieval_metrics": {
      "description": "Evaluate retrieval quality separately",
      "code_example": "from sklearn.metrics import ndcg_score, precision_score, recall_score\nimport numpy as np\n\ndef evaluate_retrieval(queries, retrieved_ids, relevant_ids, k=5):\n    metrics = []\n    \n    for query_retrieved, query_relevant in zip(retrieved_ids, relevant_ids):\n        # Precision@K\n        retrieved_k = set(query_retrieved[:k])\n        relevant_set = set(query_relevant)\n        precision_k = len(retrieved_k & relevant_set) / k\n        \n        # Recall@K\n        recall_k = len(retrieved_k & relevant_set) / len(relevant_set) if relevant_set else 0\n        \n        # MRR (Mean Reciprocal Rank)\n        mrr = 0\n        for i, doc_id in enumerate(query_retrieved):\n            if doc_id in relevant_set:\n                mrr = 1 / (i + 1)\n                break\n        \n        metrics.append({\n            'precision@k': precision_k,\n            'recall@k': recall_k,\n            'mrr': mrr\n        })\n    \n    return {\n        'precision@k': np.mean([m['precision@k'] for m in metrics]),\n        'recall@k': np.mean([m['recall@k'] for m in metrics]),\n        'mrr': np.mean([m['mrr'] for m in metrics])\n    }",
      "metrics": {
        "precision@k": "Fraction of top-k results that are relevant",
        "recall@k": "Fraction of relevant docs in top-k",
        "mrr": "Mean Reciprocal Rank of first relevant result",
        "ndcg": "Normalized Discounted Cumulative Gain"
      }
    }
  },
  "llm_evaluation": {
    "llm_as_judge": {
      "description": "Use LLM to evaluate outputs",
      "code_example": "from langchain_openai import ChatOpenAI\nfrom pydantic import BaseModel, Field\n\nclass EvaluationResult(BaseModel):\n    score: int = Field(description='Score from 1-5')\n    reasoning: str = Field(description='Explanation for score')\n\njudge_llm = ChatOpenAI(model='gpt-4o', temperature=0)\nstructured_judge = judge_llm.with_structured_output(EvaluationResult)\n\ndef evaluate_response(question: str, response: str, criteria: str) -> EvaluationResult:\n    prompt = f'''Evaluate the response on the following criteria:\n{criteria}\n\nQuestion: {question}\nResponse: {response}\n\nProvide a score (1-5) and reasoning.'''\n    \n    return structured_judge.invoke(prompt)\n\n# Evaluate on multiple criteria\ncriteria = {\n    'accuracy': 'Is the response factually correct?',\n    'completeness': 'Does the response fully address the question?',\n    'clarity': 'Is the response clear and well-structured?',\n    'helpfulness': 'Is the response helpful and actionable?'\n}\n\nfor criterion_name, criterion_desc in criteria.items():\n    result = evaluate_response(question, response, criterion_desc)\n    print(f'{criterion_name}: {result.score}/5 - {result.reasoning}')",
      "best_practices": [
        "Use GPT-4 class model for judging",
        "Temperature 0 for consistency",
        "Get structured output with reasoning",
        "Calibrate with human evaluations"
      ]
    },
    "pairwise_comparison": {
      "description": "Compare two outputs to choose better one",
      "code_example": "class ComparisonResult(BaseModel):\n    winner: str = Field(description='A, B, or TIE')\n    reasoning: str\n\ndef compare_responses(question: str, response_a: str, response_b: str) -> ComparisonResult:\n    prompt = f'''Compare these two responses to the question.\n\nQuestion: {question}\n\nResponse A: {response_a}\n\nResponse B: {response_b}\n\nWhich response is better? Choose A, B, or TIE.'''\n    \n    return structured_judge.invoke(prompt)\n\n# Use for model comparison\nwin_counts = {'A': 0, 'B': 0, 'TIE': 0}\n\nfor question in test_questions:\n    response_a = model_a.invoke(question)\n    response_b = model_b.invoke(question)\n    \n    result = compare_responses(question, response_a, response_b)\n    win_counts[result.winner] += 1\n\nprint(f'Model A wins: {win_counts[\"A\"]}')\nprint(f'Model B wins: {win_counts[\"B\"]}')\nprint(f'Ties: {win_counts[\"TIE\"]}')"
    },
    "reference_free": {
      "description": "Evaluate without ground truth",
      "code_example": "def evaluate_coherence(response: str) -> float:\n    '''Evaluate response coherence without reference.'''\n    prompt = f'''Rate the coherence of this response (1-5):\n    \n    {response}\n    \n    Consider: logical flow, consistency, structure.'''\n    \n    result = structured_judge.invoke(prompt)\n    return result.score / 5.0  # Normalize to 0-1"
    }
  },
  "automated_testing": {
    "test_suite": {
      "description": "Automated tests for LLM applications",
      "code_example": "import pytest\nfrom your_app import rag_chain\n\nclass TestRAGSystem:\n    @pytest.fixture\n    def chain(self):\n        return rag_chain\n    \n    def test_basic_query(self, chain):\n        '''Test that basic queries return responses.'''\n        response = chain.invoke('What is the return policy?')\n        assert response is not None\n        assert len(response) > 50\n    \n    def test_unknown_query(self, chain):\n        '''Test handling of unknown queries.'''\n        response = chain.invoke('What is the airspeed of an unladen swallow?')\n        # Should admit uncertainty\n        assert any(phrase in response.lower() for phrase in [\n            \"don't know\", \"not sure\", \"no information\"\n        ])\n    \n    def test_response_grounding(self, chain):\n        '''Test that response is grounded in sources.'''\n        response = chain.invoke('What are the business hours?')\n        # Check for citation or source reference\n        assert '[' in response or 'source' in response.lower()\n    \n    def test_no_hallucination(self, chain):\n        '''Test that system doesn't make things up.'''\n        response = chain.invoke('What is the CEO salary?')\n        # Should not provide confidential info not in docs\n        assert 'salary' not in response.lower() or \"don't\" in response.lower()\n    \n    @pytest.mark.parametrize('question,expected_topic', [\n        ('How do I reset my password?', 'password'),\n        ('What payment methods are accepted?', 'payment'),\n        ('How to contact support?', 'support'),\n    ])\n    def test_topic_coverage(self, chain, question, expected_topic):\n        '''Test that responses cover expected topics.'''\n        response = chain.invoke(question)\n        assert expected_topic in response.lower()",
      "best_practices": [
        "Test for both positive and negative cases",
        "Include hallucination detection tests",
        "Test edge cases and adversarial inputs",
        "Use parameterized tests for coverage"
      ]
    },
    "regression_testing": {
      "description": "Detect performance regression",
      "code_example": "import json\nfrom pathlib import Path\n\nclass RegressionTestSuite:\n    def __init__(self, baseline_path: str = 'baseline_results.json'):\n        self.baseline_path = Path(baseline_path)\n        if self.baseline_path.exists():\n            self.baseline = json.loads(self.baseline_path.read_text())\n        else:\n            self.baseline = {}\n    \n    def run_evaluation(self, chain, test_set) -> dict:\n        results = []\n        for item in test_set:\n            response = chain.invoke(item['question'])\n            score = self.evaluate(response, item['expected'])\n            results.append(score)\n        \n        return {\n            'mean_score': sum(results) / len(results),\n            'min_score': min(results),\n            'pass_rate': sum(1 for r in results if r >= 0.7) / len(results)\n        }\n    \n    def check_regression(self, current_results: dict, threshold: float = 0.05):\n        if not self.baseline:\n            print('No baseline - saving current as baseline')\n            self.save_baseline(current_results)\n            return True\n        \n        for metric, current_value in current_results.items():\n            baseline_value = self.baseline.get(metric, 0)\n            if current_value < baseline_value - threshold:\n                print(f'REGRESSION: {metric} dropped from {baseline_value:.3f} to {current_value:.3f}')\n                return False\n        \n        print('No regression detected')\n        return True\n    \n    def save_baseline(self, results: dict):\n        self.baseline_path.write_text(json.dumps(results, indent=2))"
    }
  },
  "observability": {
    "langsmith": {
      "description": "LangChain observability platform",
      "code_example": "import os\n\n# Enable LangSmith tracing\nos.environ['LANGCHAIN_TRACING_V2'] = 'true'\nos.environ['LANGCHAIN_API_KEY'] = 'your-api-key'\nos.environ['LANGCHAIN_PROJECT'] = 'my-project'\n\n# All LangChain calls automatically traced\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model='gpt-4o')\nresponse = llm.invoke('Hello!')  # Traced automatically\n\n# Manual annotation\nfrom langsmith import Client\n\nclient = Client()\n\n# Add feedback to runs\nclient.create_feedback(\n    run_id='run-id-here',\n    key='correctness',\n    score=1.0,\n    comment='Response was accurate'\n)"
    },
    "custom_logging": {
      "description": "Custom evaluation logging",
      "code_example": "import logging\nfrom datetime import datetime\nimport json\n\nclass EvaluationLogger:\n    def __init__(self, log_file: str = 'evaluations.jsonl'):\n        self.log_file = log_file\n        self.logger = logging.getLogger('evaluation')\n    \n    def log_evaluation(self, query: str, response: str, metrics: dict, metadata: dict = None):\n        entry = {\n            'timestamp': datetime.utcnow().isoformat(),\n            'query': query,\n            'response': response[:500],  # Truncate for logging\n            'metrics': metrics,\n            'metadata': metadata or {}\n        }\n        \n        with open(self.log_file, 'a') as f:\n            f.write(json.dumps(entry) + '\\n')\n        \n        # Alert on poor scores\n        if any(v < 0.5 for v in metrics.values()):\n            self.logger.warning(f'Low score detected: {metrics}')\n\n# Usage\nlogger = EvaluationLogger()\nlogger.log_evaluation(\n    query='What is the return policy?',\n    response='Our return policy allows...',\n    metrics={'faithfulness': 0.9, 'relevancy': 0.85}\n)"
    }
  },
  "benchmark_patterns": {
    "create_benchmark": {
      "description": "Create custom benchmark dataset",
      "code_example": "from dataclasses import dataclass\nfrom typing import List, Optional\nimport json\n\n@dataclass\nclass BenchmarkItem:\n    id: str\n    question: str\n    expected_answer: str\n    category: str\n    difficulty: str\n    metadata: dict\n\nclass Benchmark:\n    def __init__(self, name: str, items: List[BenchmarkItem]):\n        self.name = name\n        self.items = items\n    \n    @classmethod\n    def from_file(cls, path: str) -> 'Benchmark':\n        data = json.loads(Path(path).read_text())\n        items = [BenchmarkItem(**item) for item in data['items']]\n        return cls(data['name'], items)\n    \n    def run(self, model_fn, evaluator_fn) -> dict:\n        results = []\n        for item in self.items:\n            response = model_fn(item.question)\n            score = evaluator_fn(response, item.expected_answer)\n            results.append({\n                'id': item.id,\n                'category': item.category,\n                'difficulty': item.difficulty,\n                'score': score\n            })\n        \n        return self.aggregate_results(results)\n    \n    def aggregate_results(self, results: List[dict]) -> dict:\n        import pandas as pd\n        df = pd.DataFrame(results)\n        \n        return {\n            'overall': df['score'].mean(),\n            'by_category': df.groupby('category')['score'].mean().to_dict(),\n            'by_difficulty': df.groupby('difficulty')['score'].mean().to_dict()\n        }"
    }
  },
  "anti_patterns": {
    "no_evaluation": {
      "description": "Deploying without evaluation",
      "problem": "Unknown quality, silent failures",
      "solution": "Always evaluate before deployment"
    },
    "wrong_metrics": {
      "description": "Using irrelevant metrics",
      "problem": "Optimizing for wrong thing",
      "solution": "Choose metrics aligned with user needs"
    },
    "static_evaluation": {
      "description": "Only evaluating once",
      "problem": "Missing drift and degradation",
      "solution": "Continuous monitoring and evaluation"
    },
    "biased_test_set": {
      "description": "Non-representative test data",
      "problem": "False confidence in model",
      "solution": "Use diverse, representative test sets"
    }
  },
  "best_practices_summary": [
    "Evaluate before deploying any changes",
    "Use multiple metrics for comprehensive view",
    "Include both automated and human evaluation",
    "Set performance thresholds and alerts",
    "Track metrics over time for drift detection",
    "Test adversarial and edge cases",
    "Calibrate LLM-as-judge with human ratings",
    "Document evaluation methodology"
  ]
}
