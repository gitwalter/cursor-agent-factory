{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "LLM Fine-Tuning Patterns",
  "description": "Patterns for fine-tuning large language models",
  "version": "1.0.0",
  "axiomAlignment": {
    "A1_verifiability": "Fine-tuning includes evaluation to verify improvement",
    "A3_transparency": "Training data and methodology are documented"
  },
  "fine_tuning_methods": {
    "comparison": {
      "full_fine_tuning": {
        "description": "Train all model parameters",
        "memory": "Very High (4-8x model size)",
        "quality": "Best",
        "use_when": "Sufficient compute, major domain shift"
      },
      "lora": {
        "description": "Low-Rank Adaptation",
        "memory": "Low (fraction of model)",
        "quality": "Very Good",
        "use_when": "Limited GPU, most use cases"
      },
      "qlora": {
        "description": "Quantized LoRA",
        "memory": "Very Low",
        "quality": "Good",
        "use_when": "Very limited GPU, consumer hardware"
      },
      "adapter": {
        "description": "Adapter layers",
        "memory": "Low",
        "quality": "Good",
        "use_when": "Task-specific adaptation"
      }
    },
    "selection_guide": {
      "consumer_gpu_8gb": "QLoRA with 4-bit quantization",
      "consumer_gpu_24gb": "LoRA with 16-bit",
      "professional_gpu_80gb": "Full fine-tuning or LoRA",
      "multi_gpu": "Full fine-tuning with DeepSpeed/FSDP"
    }
  },
  "data_preparation": {
    "instruction_format": {
      "description": "Format data for instruction tuning",
      "code_example": "# Standard instruction format\ninstruction_template = '''### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\n{output}'''\n\n# Chat format (preferred for chat models)\nchat_template = [\n    {'role': 'system', 'content': 'You are a helpful assistant.'},\n    {'role': 'user', 'content': '{instruction}'},\n    {'role': 'assistant', 'content': '{output}'}\n]\n\n# Prepare dataset\nfrom datasets import Dataset\n\ndef format_instruction(example):\n    return {\n        'text': instruction_template.format(\n            instruction=example['instruction'],\n            input=example.get('input', ''),\n            output=example['output']\n        )\n    }\n\ndataset = Dataset.from_json('data.jsonl')\nformatted_dataset = dataset.map(format_instruction)\n\n# For chat models\ndef format_chat(example):\n    messages = [\n        {'role': 'user', 'content': example['instruction']},\n        {'role': 'assistant', 'content': example['output']}\n    ]\n    return {'messages': messages}",
      "data_quality": [
        "Diverse examples covering use cases",
        "High-quality outputs (human verified)",
        "Balanced distribution across categories",
        "Remove duplicates and near-duplicates",
        "Include edge cases"
      ]
    },
    "data_size": {
      "guidelines": {
        "minimum": "100-500 examples for simple tasks",
        "recommended": "1000-5000 examples for good quality",
        "large_scale": "10000+ for complex tasks or domain shift"
      }
    }
  },
  "lora_patterns": {
    "basic_lora": {
      "description": "Standard LoRA fine-tuning",
      "code_example": "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom trl import SFTTrainer\nfrom datasets import load_dataset\nimport torch\n\n# Load base model\nmodel_name = 'meta-llama/Llama-3.2-1B-Instruct'\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map='auto'\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n# LoRA configuration\nlora_config = LoraConfig(\n    r=16,  # Rank - higher = more parameters\n    lora_alpha=32,  # Scaling factor\n    lora_dropout=0.05,\n    bias='none',\n    task_type=TaskType.CAUSAL_LM,\n    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n)\n\n# Apply LoRA\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./lora_output',\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    lr_scheduler_type='cosine',\n    warmup_ratio=0.03,\n    fp16=True,\n    logging_steps=10,\n    save_strategy='epoch',\n    eval_strategy='epoch',\n    save_total_limit=3,\n    load_best_model_at_end=True\n)\n\n# SFT Trainer\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    max_seq_length=2048,\n    dataset_text_field='text'\n)\n\n# Train\ntrainer.train()\n\n# Save adapter\nmodel.save_pretrained('./lora_adapter')",
      "hyperparameters": {
        "r": "8-64, start with 16",
        "lora_alpha": "Usually 2x r",
        "learning_rate": "1e-4 to 3e-4",
        "epochs": "1-5 typically sufficient"
      }
    },
    "qlora": {
      "description": "Quantized LoRA for consumer GPUs",
      "code_example": "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport torch\n\n# 4-bit quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True\n)\n\n# Load quantized model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map='auto'\n)\n\n# Prepare for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Apply LoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias='none',\n    task_type=TaskType.CAUSAL_LM,\n    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj']\n)\n\nmodel = get_peft_model(model, lora_config)\n\n# Rest of training same as standard LoRA"
    }
  },
  "full_fine_tuning": {
    "deepspeed": {
      "description": "Full fine-tuning with DeepSpeed",
      "code_example": "# ds_config.json\n{\n  \"bf16\": {\"enabled\": true},\n  \"zero_optimization\": {\n    \"stage\": 2,\n    \"offload_optimizer\": {\"device\": \"cpu\"},\n    \"allgather_partitions\": true,\n    \"reduce_scatter\": true\n  },\n  \"gradient_accumulation_steps\": 8,\n  \"train_micro_batch_size_per_gpu\": 2,\n  \"wall_clock_breakdown\": false\n}\n\n# Training script\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./full_ft_output',\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=8,\n    learning_rate=2e-5,\n    bf16=True,\n    deepspeed='ds_config.json',\n    save_strategy='steps',\n    save_steps=500,\n    logging_steps=10\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator\n)\n\ntrainer.train()"
    }
  },
  "specialized_tuning": {
    "dpo": {
      "description": "Direct Preference Optimization",
      "use_when": "Align model to preferences without reward model",
      "code_example": "from trl import DPOTrainer, DPOConfig\nfrom peft import LoraConfig\n\n# Preference data format\n# {'prompt': '...', 'chosen': '...', 'rejected': '...'}\n\ndpo_config = DPOConfig(\n    beta=0.1,  # KL penalty\n    learning_rate=5e-6,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    num_train_epochs=1,\n    bf16=True\n)\n\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=None,  # Will use initial model as reference\n    args=dpo_config,\n    train_dataset=preference_dataset,\n    tokenizer=tokenizer,\n    peft_config=lora_config  # Optional: use with LoRA\n)\n\ntrainer.train()"
    },
    "orpo": {
      "description": "Odds Ratio Preference Optimization",
      "use_when": "Preference tuning without reference model",
      "code_example": "from trl import ORPOTrainer, ORPOConfig\n\norpo_config = ORPOConfig(\n    learning_rate=1e-5,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    num_train_epochs=1,\n    beta=0.1,\n    bf16=True\n)\n\ntrainer = ORPOTrainer(\n    model=model,\n    args=orpo_config,\n    train_dataset=preference_dataset,\n    tokenizer=tokenizer,\n    peft_config=lora_config\n)\n\ntrainer.train()"
    }
  },
  "evaluation": {
    "during_training": {
      "description": "Monitor training progress",
      "code_example": "from transformers import TrainerCallback\nimport wandb\n\nclass EvaluationCallback(TrainerCallback):\n    def __init__(self, eval_prompts):\n        self.eval_prompts = eval_prompts\n    \n    def on_epoch_end(self, args, state, control, model, tokenizer, **kwargs):\n        model.eval()\n        results = []\n        \n        for prompt in self.eval_prompts:\n            inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n            with torch.no_grad():\n                outputs = model.generate(**inputs, max_new_tokens=100)\n            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n            results.append({'prompt': prompt, 'response': response})\n        \n        # Log to wandb\n        wandb.log({'eval_samples': wandb.Table(dataframe=pd.DataFrame(results))})"
    },
    "post_training": {
      "description": "Evaluate fine-tuned model",
      "code_example": "from lm_eval import evaluator\n\n# Run standard benchmarks\nresults = evaluator.simple_evaluate(\n    model='hf',\n    model_args=f'pretrained={model_path}',\n    tasks=['hellaswag', 'arc_easy', 'arc_challenge'],\n    num_fewshot=0,\n    batch_size='auto'\n)\n\nprint(results['results'])"
    }
  },
  "merging_deployment": {
    "merge_lora": {
      "description": "Merge LoRA weights into base model",
      "code_example": "from peft import PeftModel\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    'meta-llama/Llama-3.2-1B-Instruct',\n    torch_dtype=torch.float16,\n    device_map='auto'\n)\n\n# Load and merge LoRA\npeft_model = PeftModel.from_pretrained(base_model, './lora_adapter')\nmerged_model = peft_model.merge_and_unload()\n\n# Save merged model\nmerged_model.save_pretrained('./merged_model')\ntokenizer.save_pretrained('./merged_model')"
    },
    "push_to_hub": {
      "description": "Upload model to Hugging Face Hub",
      "code_example": "from huggingface_hub import login\n\nlogin(token='your_token')\n\n# Push merged model\nmerged_model.push_to_hub('username/my-fine-tuned-model')\ntokenizer.push_to_hub('username/my-fine-tuned-model')\n\n# Or push just adapter\npeft_model.push_to_hub('username/my-lora-adapter')"
    }
  },
  "anti_patterns": {
    "too_small_dataset": {
      "description": "Fine-tuning with very few examples",
      "problem": "Overfitting, poor generalization",
      "solution": "Collect more data or use few-shot prompting"
    },
    "learning_rate_too_high": {
      "description": "Using high learning rate",
      "problem": "Catastrophic forgetting, instability",
      "solution": "Use lower LR (1e-5 to 3e-4)"
    },
    "no_validation": {
      "description": "Training without validation set",
      "problem": "Cannot detect overfitting",
      "solution": "Always hold out validation data"
    },
    "wrong_format": {
      "description": "Wrong instruction format for model",
      "problem": "Poor results, confused model",
      "solution": "Use model's native chat template"
    }
  },
  "best_practices_summary": [
    "Start with LoRA for most use cases",
    "Use QLoRA for limited GPU memory",
    "Prepare high-quality training data",
    "Use model's native chat template",
    "Set appropriate learning rate (1e-5 to 3e-4)",
    "Monitor validation loss for overfitting",
    "Evaluate on held-out test set",
    "Merge and push to Hub for deployment"
  ]
}
