{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "RAG Patterns",
  "description": "Retrieval-Augmented Generation patterns for building production RAG systems",
  "version": "1.0.0",
  "axiomAlignment": {
    "A1_verifiability": "RAG grounds responses in source documents",
    "A3_transparency": "Citations enable fact-checking"
  },
  "architecture_patterns": {
    "basic_rag": {
      "description": "Simple retrieve-then-generate pipeline",
      "use_when": "Starting with RAG, simple use cases",
      "components": ["Document Loader", "Text Splitter", "Embeddings", "Vector Store", "Retriever", "LLM"],
      "code_example": "from langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\n\n# 1. Load documents\nloader = PyPDFLoader('document.pdf')\ndocs = loader.load()\n\n# 2. Split into chunks\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200\n)\nchunks = text_splitter.split_documents(docs)\n\n# 3. Create embeddings and store\nembeddings = OpenAIEmbeddings(model='text-embedding-3-small')\nvectorstore = Chroma.from_documents(chunks, embeddings)\nretriever = vectorstore.as_retriever(search_kwargs={'k': 5})\n\n# 4. Create RAG chain\nllm = ChatOpenAI(model='gpt-4o')\n\nprompt = ChatPromptTemplate.from_template('''\nAnswer the question based on the context below.\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:''')\n\ndef format_docs(docs):\n    return '\\n\\n'.join(doc.page_content for doc in docs)\n\nrag_chain = (\n    {'context': retriever | format_docs, 'question': RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\n# Query\nresponse = rag_chain.invoke('What is the main topic?')\nprint(response)",
      "best_practices": [
        "Start simple, add complexity as needed",
        "Log retrieved chunks for debugging",
        "Include source attribution in responses"
      ]
    },
    "conversational_rag": {
      "description": "RAG with conversation history",
      "use_when": "Building chatbots that need document context",
      "code_example": "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.messages import HumanMessage, AIMessage\nfrom langchain.chains import create_history_aware_retriever, create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\n\n# Contextualize question prompt\ncontextualize_prompt = ChatPromptTemplate.from_messages([\n    ('system', '''Given a chat history and the latest user question,\n    formulate a standalone question that can be understood without\n    the chat history. Do NOT answer the question, just reformulate\n    it if needed, otherwise return it as is.'''),\n    MessagesPlaceholder('chat_history'),\n    ('human', '{input}')\n])\n\n# Create history-aware retriever\nhistory_aware_retriever = create_history_aware_retriever(\n    llm, retriever, contextualize_prompt\n)\n\n# QA prompt\nqa_prompt = ChatPromptTemplate.from_messages([\n    ('system', '''Answer the question based on the context.\n    \n    Context: {context}'''),\n    MessagesPlaceholder('chat_history'),\n    ('human', '{input}')\n])\n\n# Create chain\nqa_chain = create_stuff_documents_chain(llm, qa_prompt)\nrag_chain = create_retrieval_chain(history_aware_retriever, qa_chain)\n\n# Usage with history\nchat_history = []\n\nresponse = rag_chain.invoke({\n    'input': 'What is the main topic?',\n    'chat_history': chat_history\n})\n\nchat_history.extend([\n    HumanMessage(content='What is the main topic?'),\n    AIMessage(content=response['answer'])\n])\n\n# Follow-up question\nresponse = rag_chain.invoke({\n    'input': 'Tell me more about that',\n    'chat_history': chat_history\n})",
      "best_practices": [
        "Reformulate questions using history",
        "Limit history length to avoid token limits",
        "Clear history for new conversations"
      ]
    },
    "agentic_rag": {
      "description": "RAG with agent for complex queries",
      "use_when": "Need routing, multi-step retrieval, or tools",
      "code_example": "from langchain.agents import create_tool_calling_agent, AgentExecutor\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n# Define retrieval tools\n@tool\ndef search_documents(query: str) -> str:\n    '''Search the document database for relevant information.'''\n    docs = retriever.invoke(query)\n    return '\\n\\n'.join(doc.page_content for doc in docs)\n\n@tool  \ndef search_web(query: str) -> str:\n    '''Search the web for current information.'''\n    # Implement web search\n    return web_search(query)\n\ntools = [search_documents, search_web]\n\n# Create agent\nllm = ChatOpenAI(model='gpt-4o', temperature=0)\n\nprompt = ChatPromptTemplate.from_messages([\n    ('system', '''You are a helpful assistant with access to tools.\n    Use search_documents for questions about our documentation.\n    Use search_web for current events or external information.'''),\n    ('human', '{input}'),\n    MessagesPlaceholder('agent_scratchpad')\n])\n\nagent = create_tool_calling_agent(llm, tools, prompt)\nexecutor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nresponse = executor.invoke({'input': 'What does our policy say about refunds?'})",
      "best_practices": [
        "Use agents when routing between sources",
        "Keep tool descriptions clear",
        "Add fallback for no relevant results"
      ]
    }
  },
  "chunking_strategies": {
    "recursive_character": {
      "description": "Split by character with hierarchy",
      "use_when": "General text documents",
      "code_example": "from langchain.text_splitter import RecursiveCharacterTextSplitter\n\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n    length_function=len,\n    separators=['\\n\\n', '\\n', '. ', ' ', '']\n)\n\nchunks = splitter.split_documents(docs)",
      "parameters": {
        "chunk_size": "500-1500 tokens recommended",
        "chunk_overlap": "10-20% of chunk_size",
        "separators": "Order from most to least preferred"
      }
    },
    "semantic_chunking": {
      "description": "Split based on semantic similarity",
      "use_when": "Need semantically coherent chunks",
      "code_example": "from langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings()\nchunker = SemanticChunker(\n    embeddings,\n    breakpoint_threshold_type='percentile',\n    breakpoint_threshold_amount=95\n)\n\nchunks = chunker.split_documents(docs)"
    },
    "parent_document": {
      "description": "Store small chunks, retrieve larger parents",
      "use_when": "Need precise matching with broader context",
      "code_example": "from langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Small chunks for retrieval\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n# Large chunks for context\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter\n)\n\nretriever.add_documents(docs)"
    }
  },
  "retrieval_strategies": {
    "similarity": {
      "description": "Basic vector similarity search",
      "code_example": "retriever = vectorstore.as_retriever(\n    search_type='similarity',\n    search_kwargs={'k': 5}\n)"
    },
    "mmr": {
      "description": "Maximum Marginal Relevance for diversity",
      "code_example": "retriever = vectorstore.as_retriever(\n    search_type='mmr',\n    search_kwargs={\n        'k': 5,\n        'fetch_k': 20,\n        'lambda_mult': 0.5\n    }\n)"
    },
    "hybrid": {
      "description": "Combine vector and keyword search",
      "code_example": "from langchain.retrievers import EnsembleRetriever\nfrom langchain_community.retrievers import BM25Retriever\n\n# BM25 for keyword matching\nbm25_retriever = BM25Retriever.from_documents(docs)\nbm25_retriever.k = 5\n\n# Vector for semantic\nvector_retriever = vectorstore.as_retriever(search_kwargs={'k': 5})\n\n# Combine with weights\nensemble_retriever = EnsembleRetriever(\n    retrievers=[bm25_retriever, vector_retriever],\n    weights=[0.3, 0.7]  # Prefer semantic\n)"
    },
    "self_query": {
      "description": "LLM generates metadata filters from query",
      "code_example": "from langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain.chains.query_constructor.base import AttributeInfo\n\nmetadata_field_info = [\n    AttributeInfo(name='source', description='Document source', type='string'),\n    AttributeInfo(name='date', description='Publication date', type='date'),\n    AttributeInfo(name='category', description='Category', type='string')\n]\n\nretriever = SelfQueryRetriever.from_llm(\n    llm=llm,\n    vectorstore=vectorstore,\n    document_contents='Technical documentation',\n    metadata_field_info=metadata_field_info\n)"
    },
    "contextual_compression": {
      "description": "Extract relevant parts from retrieved docs",
      "code_example": "from langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\n\ncompressor = LLMChainExtractor.from_llm(llm)\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=retriever\n)"
    },
    "reranking": {
      "description": "Re-score results with cross-encoder",
      "code_example": "from langchain.retrievers import ContextualCompressionRetriever\nfrom langchain_cohere import CohereRerank\n\nreranker = CohereRerank(top_n=5)\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=reranker,\n    base_retriever=retriever\n)"
    }
  },
  "prompt_patterns": {
    "basic_qa": {
      "template": "Answer the question based on the context below. If the answer is not in the context, say 'I don't know.'\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:"
    },
    "with_citations": {
      "template": "Answer the question based on the context. Include citations in [Source N] format.\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer with citations:"
    },
    "structured_output": {
      "code_example": "from pydantic import BaseModel, Field\nfrom langchain_openai import ChatOpenAI\n\nclass Answer(BaseModel):\n    answer: str = Field(description='The answer to the question')\n    sources: list[str] = Field(description='Source documents used')\n    confidence: float = Field(description='Confidence score 0-1')\n\nllm = ChatOpenAI(model='gpt-4o')\nstructured_llm = llm.with_structured_output(Answer)\n\nresponse = structured_llm.invoke(\n    f'Context: {context}\\n\\nQuestion: {question}'\n)"
    }
  },
  "evaluation_patterns": {
    "ragas": {
      "description": "Evaluate RAG with RAGAS framework",
      "code_example": "from ragas import evaluate\nfrom ragas.metrics import (\n    faithfulness,\n    answer_relevancy,\n    context_precision,\n    context_recall\n)\nfrom datasets import Dataset\n\n# Prepare evaluation data\neval_data = {\n    'question': ['What is X?', 'How does Y work?'],\n    'answer': [rag_answer_1, rag_answer_2],\n    'contexts': [[ctx1, ctx2], [ctx3, ctx4]],\n    'ground_truth': ['X is...', 'Y works by...']\n}\n\ndataset = Dataset.from_dict(eval_data)\n\n# Evaluate\nresults = evaluate(\n    dataset,\n    metrics=[\n        faithfulness,\n        answer_relevancy,\n        context_precision,\n        context_recall\n    ]\n)\n\nprint(results)",
      "metrics": {
        "faithfulness": "Is answer grounded in context?",
        "answer_relevancy": "Is answer relevant to question?",
        "context_precision": "Are retrieved contexts relevant?",
        "context_recall": "Did we retrieve all needed context?"
      }
    },
    "custom_evaluation": {
      "description": "Custom evaluation metrics",
      "code_example": "from langchain_openai import ChatOpenAI\n\ndef evaluate_answer(question, answer, context, ground_truth):\n    llm = ChatOpenAI(model='gpt-4o', temperature=0)\n    \n    prompt = f'''Evaluate the answer on a scale of 1-5:\n    \n    Question: {question}\n    Context: {context}\n    Answer: {answer}\n    Expected: {ground_truth}\n    \n    Score (1-5) and explanation:'''\n    \n    response = llm.invoke(prompt)\n    return response.content"
    }
  },
  "production_patterns": {
    "caching": {
      "description": "Cache embeddings and responses",
      "code_example": "from langchain.cache import SQLiteCache\nfrom langchain.globals import set_llm_cache\n\n# LLM response cache\nset_llm_cache(SQLiteCache(database_path='.langchain.db'))\n\n# Embedding cache\nfrom langchain.embeddings import CacheBackedEmbeddings\nfrom langchain.storage import LocalFileStore\n\nstore = LocalFileStore('./embedding_cache')\ncached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n    underlying_embeddings=embeddings,\n    document_embedding_cache=store,\n    namespace='embeddings'\n)"
    },
    "streaming": {
      "description": "Stream responses for better UX",
      "code_example": "from langchain_core.output_parsers import StrOutputParser\n\nrag_chain = (\n    {'context': retriever | format_docs, 'question': RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\n# Stream response\nfor chunk in rag_chain.stream('What is the main topic?'):\n    print(chunk, end='', flush=True)"
    },
    "observability": {
      "description": "Monitor RAG in production",
      "code_example": "import os\nos.environ['LANGCHAIN_TRACING_V2'] = 'true'\nos.environ['LANGCHAIN_API_KEY'] = 'your-key'\nos.environ['LANGCHAIN_PROJECT'] = 'rag-production'\n\n# All chains automatically traced to LangSmith"
    }
  },
  "anti_patterns": {
    "no_chunking": {
      "description": "Using whole documents without chunking",
      "problem": "Poor retrieval, token limit issues",
      "solution": "Chunk documents appropriately"
    },
    "wrong_embedding_model": {
      "description": "Using different models for indexing and query",
      "problem": "Semantic mismatch, bad results",
      "solution": "Use same embedding model throughout"
    },
    "no_evaluation": {
      "description": "Deploying without evaluation",
      "problem": "Unknown quality, silent failures",
      "solution": "Evaluate with RAGAS before deployment"
    },
    "hallucination_risk": {
      "description": "Not instructing model to admit uncertainty",
      "problem": "Confident wrong answers",
      "solution": "Prompt to say 'I don't know' when unsure"
    }
  },
  "best_practices_summary": [
    "Chunk documents appropriately (500-1500 tokens)",
    "Use overlap between chunks (10-20%)",
    "Store rich metadata with chunks",
    "Consider hybrid search for production",
    "Implement reranking for precision",
    "Include source citations in responses",
    "Evaluate with RAGAS before deployment",
    "Enable tracing for debugging",
    "Cache embeddings and responses",
    "Stream for better user experience"
  ]
}
