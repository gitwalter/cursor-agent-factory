{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "ML Workflow Patterns",
  "description": "Framework-agnostic machine learning workflow patterns for data preparation, model training, evaluation, and deployment",
  "version": "1.0.0",
  "axiomAlignment": {
    "A1_verifiability": "All patterns include metrics and validation steps for reproducibility",
    "A3_transparency": "Patterns document assumptions and data transformations explicitly"
  },
  "data_preparation": {
    "train_test_split": {
      "description": "Split data into training and testing sets with stratification",
      "use_when": "Starting any supervised learning project",
      "code_example": "from sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('data.csv')\nX = df.drop('target', axis=1)\ny = df['target']\n\n# Stratified split for classification\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y  # Maintains class distribution\n)\n\n# For regression (no stratification)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42\n)\n\nprint(f'Train size: {len(X_train)}, Test size: {len(X_test)}')",
      "best_practices": [
        "Always set random_state for reproducibility",
        "Use stratify for imbalanced classification",
        "Typical splits: 80/20 or 70/30 for small datasets",
        "Consider temporal splits for time-series data",
        "Never look at test data during model development"
      ]
    },
    "cross_validation": {
      "description": "K-fold cross-validation for robust model evaluation",
      "use_when": "Need reliable performance estimates, especially with limited data",
      "code_example": "from sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\n# Define model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Simple cross-validation\nscores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\nprint(f'CV Accuracy: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})')\n\n# Stratified K-Fold for more control\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfold_scores = []\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n    \n    model.fit(X_train_fold, y_train_fold)\n    score = model.score(X_val_fold, y_val_fold)\n    fold_scores.append(score)\n    print(f'Fold {fold + 1}: {score:.3f}')\n\nprint(f'Mean CV Score: {np.mean(fold_scores):.3f}')",
      "best_practices": [
        "Use 5 or 10 folds as standard",
        "Use StratifiedKFold for classification",
        "Use TimeSeriesSplit for temporal data",
        "Report mean and standard deviation",
        "Use same CV strategy for all model comparisons"
      ]
    },
    "feature_scaling": {
      "description": "Normalize or standardize features for model training",
      "use_when": "Using distance-based or gradient-based algorithms",
      "code_example": "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.compose import ColumnTransformer\nimport pandas as pd\n\n# Identify column types\nnumeric_cols = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n\n# StandardScaler: zero mean, unit variance (most common)\nstandard_scaler = StandardScaler()\nX_scaled = standard_scaler.fit_transform(X[numeric_cols])\n\n# MinMaxScaler: scale to [0, 1] range\nminmax_scaler = MinMaxScaler()\nX_normalized = minmax_scaler.fit_transform(X[numeric_cols])\n\n# RobustScaler: handles outliers better\nrobust_scaler = RobustScaler()\nX_robust = robust_scaler.fit_transform(X[numeric_cols])\n\n# IMPORTANT: Fit on training data only!\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train[numeric_cols])  # fit_transform\nX_test_scaled = scaler.transform(X_test[numeric_cols])  # transform only",
      "best_practices": [
        "Always fit scaler on training data only",
        "Use StandardScaler for most algorithms",
        "Use MinMaxScaler for neural networks with bounded activations",
        "Use RobustScaler when data has outliers",
        "Tree-based models don't require scaling"
      ]
    },
    "handling_missing_values": {
      "description": "Strategies for dealing with missing data",
      "use_when": "Dataset contains null or missing values",
      "code_example": "from sklearn.impute import SimpleImputer, KNNImputer\nimport pandas as pd\nimport numpy as np\n\n# Check missing values\nprint(df.isnull().sum())\nprint(f'Total missing: {df.isnull().sum().sum()}')\n\n# Simple imputation strategies\nmean_imputer = SimpleImputer(strategy='mean')  # For numeric\nmedian_imputer = SimpleImputer(strategy='median')  # Robust to outliers\nmode_imputer = SimpleImputer(strategy='most_frequent')  # For categorical\n\n# Apply imputation\nX_numeric = df[numeric_cols]\nX_imputed = pd.DataFrame(\n    mean_imputer.fit_transform(X_numeric),\n    columns=numeric_cols\n)\n\n# KNN Imputation (more sophisticated)\nknn_imputer = KNNImputer(n_neighbors=5)\nX_knn_imputed = knn_imputer.fit_transform(X_numeric)\n\n# Drop rows with missing target\ndf_clean = df.dropna(subset=['target'])",
      "best_practices": [
        "Understand WHY data is missing (MCAR, MAR, MNAR)",
        "Use median for skewed distributions",
        "Consider domain knowledge for imputation",
        "Document imputation decisions",
        "Never impute target variable - drop those rows"
      ]
    },
    "handling_categorical": {
      "description": "Encode categorical variables for ML algorithms",
      "use_when": "Dataset contains categorical features",
      "code_example": "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nimport pandas as pd\n\n# Identify categorical columns\ncategorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n\n# One-Hot Encoding (nominal categories)\nonehot = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\nX_onehot = onehot.fit_transform(X[categorical_cols])\n\n# Ordinal Encoding (ordered categories)\nordinal = OrdinalEncoder(categories=[['low', 'medium', 'high']])\nX_ordinal = ordinal.fit_transform(X[['priority']])\n\n# Label Encoding (for target variable)\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Combined preprocessing with ColumnTransformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numeric_cols),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n    ]\n)\n\nX_processed = preprocessor.fit_transform(X)",
      "best_practices": [
        "Use OneHotEncoder for nominal categories (no order)",
        "Use OrdinalEncoder for ordered categories",
        "Set handle_unknown='ignore' for unseen categories",
        "Consider target encoding for high-cardinality features",
        "Use LabelEncoder only for target variable"
      ]
    },
    "handling_imbalanced_data": {
      "description": "Techniques for dealing with class imbalance",
      "use_when": "One class significantly outnumbers others",
      "code_example": "from sklearn.utils.class_weight import compute_class_weight\nfrom imblearn.over_sampling import SMOTE, RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nimport numpy as np\n\n# Check class distribution\nprint(y.value_counts(normalize=True))\n\n# Method 1: Class weights\nclass_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\nweight_dict = dict(zip(np.unique(y), class_weights))\n\n# Use with model\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(class_weight='balanced')  # or class_weight=weight_dict\n\n# Method 2: SMOTE (Synthetic Minority Oversampling)\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n\n# Method 3: Combined pipeline\nfrom imblearn.combine import SMOTETomek\npipeline = ImbPipeline([\n    ('smote', SMOTE(random_state=42)),\n    ('classifier', RandomForestClassifier())\n])",
      "best_practices": [
        "Check class distribution before modeling",
        "Use class_weight='balanced' as first approach",
        "Apply SMOTE only to training data",
        "Use appropriate metrics (F1, AUC-ROC, not accuracy)",
        "Consider cost-sensitive learning for business context"
      ]
    }
  },
  "model_selection": {
    "hyperparameter_tuning": {
      "description": "Optimize model hyperparameters systematically",
      "use_when": "Need to find optimal model configuration",
      "code_example": "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\n# Grid Search (exhaustive)\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [10, 20, 30, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\ngrid_search = GridSearchCV(\n    RandomForestClassifier(random_state=42),\n    param_grid,\n    cv=5,\n    scoring='f1_weighted',\n    n_jobs=-1,\n    verbose=1\n)\ngrid_search.fit(X_train, y_train)\n\nprint(f'Best params: {grid_search.best_params_}')\nprint(f'Best score: {grid_search.best_score_:.3f}')\n\n# Random Search (faster for large param spaces)\nparam_distributions = {\n    'n_estimators': np.arange(50, 500, 50),\n    'max_depth': [5, 10, 20, 30, None],\n    'min_samples_split': np.arange(2, 20),\n    'min_samples_leaf': np.arange(1, 10)\n}\n\nrandom_search = RandomizedSearchCV(\n    RandomForestClassifier(random_state=42),\n    param_distributions,\n    n_iter=50,\n    cv=5,\n    scoring='f1_weighted',\n    n_jobs=-1,\n    random_state=42\n)\nrandom_search.fit(X_train, y_train)",
      "best_practices": [
        "Start with RandomizedSearchCV for large param spaces",
        "Use GridSearchCV for final fine-tuning",
        "Always use cross-validation (cv parameter)",
        "Set n_jobs=-1 to use all CPU cores",
        "Choose scoring metric aligned with business goal"
      ]
    },
    "model_comparison": {
      "description": "Compare multiple models systematically",
      "use_when": "Need to select best algorithm for the task",
      "code_example": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nimport pandas as pd\n\n# Define models to compare\nmodels = {\n    'Logistic Regression': LogisticRegression(max_iter=1000),\n    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n    'SVM': SVC(kernel='rbf', random_state=42)\n}\n\n# Compare with cross-validation\nresults = []\nfor name, model in models.items():\n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1_weighted')\n    results.append({\n        'Model': name,\n        'Mean F1': scores.mean(),\n        'Std F1': scores.std(),\n        'Min F1': scores.min(),\n        'Max F1': scores.max()\n    })\n    print(f'{name}: {scores.mean():.3f} (+/- {scores.std():.3f})')\n\n# Create comparison DataFrame\ncomparison_df = pd.DataFrame(results).sort_values('Mean F1', ascending=False)\nprint(comparison_df)",
      "best_practices": [
        "Compare at least 3-5 different algorithms",
        "Use same CV splits for fair comparison",
        "Report mean and standard deviation",
        "Consider training time for production",
        "Include simple baseline (e.g., DummyClassifier)"
      ]
    },
    "pipeline_pattern": {
      "description": "Create end-to-end preprocessing and modeling pipeline",
      "use_when": "Building production-ready ML workflow",
      "code_example": "from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Define column types\nnumeric_features = ['age', 'income', 'score']\ncategorical_features = ['category', 'region']\n\n# Numeric preprocessing\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\n# Categorical preprocessing\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessors\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ]\n)\n\n# Full pipeline\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n])\n\n# Fit and predict\npipeline.fit(X_train, y_train)\npredictions = pipeline.predict(X_test)\n\n# Save pipeline\nimport joblib\njoblib.dump(pipeline, 'model_pipeline.joblib')",
      "best_practices": [
        "Use Pipeline for reproducible workflows",
        "Include all preprocessing in pipeline",
        "Pipeline prevents data leakage automatically",
        "Easy to serialize and deploy",
        "Compatible with GridSearchCV"
      ]
    }
  },
  "evaluation_metrics": {
    "classification_metrics": {
      "description": "Metrics for evaluating classification models",
      "use_when": "Evaluating classification performance",
      "code_example": "from sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    classification_report, confusion_matrix, roc_auc_score\n)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Get predictions\ny_pred = model.predict(X_test)\ny_proba = model.predict_proba(X_test)[:, 1]  # For binary classification\n\n# Basic metrics\nprint(f'Accuracy: {accuracy_score(y_test, y_pred):.3f}')\nprint(f'Precision: {precision_score(y_test, y_pred, average=\"weighted\"):.3f}')\nprint(f'Recall: {recall_score(y_test, y_pred, average=\"weighted\"):.3f}')\nprint(f'F1 Score: {f1_score(y_test, y_pred, average=\"weighted\"):.3f}')\nprint(f'ROC-AUC: {roc_auc_score(y_test, y_proba):.3f}')\n\n# Detailed report\nprint('\\nClassification Report:')\nprint(classification_report(y_test, y_pred))\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.savefig('confusion_matrix.png')",
      "best_practices": [
        "Use F1 for imbalanced classes (not accuracy)",
        "Use ROC-AUC for ranking ability",
        "Report precision AND recall for business context",
        "Always show confusion matrix",
        "Use 'weighted' average for multiclass"
      ],
      "metric_selection_guide": {
        "balanced_classes": "Accuracy is acceptable",
        "imbalanced_classes": "Use F1, Precision, Recall, or ROC-AUC",
        "cost_of_false_positive_high": "Optimize for Precision",
        "cost_of_false_negative_high": "Optimize for Recall",
        "ranking_matters": "Use ROC-AUC or PR-AUC"
      }
    },
    "regression_metrics": {
      "description": "Metrics for evaluating regression models",
      "use_when": "Evaluating regression performance",
      "code_example": "from sklearn.metrics import (\n    mean_squared_error, mean_absolute_error, r2_score,\n    mean_absolute_percentage_error\n)\nimport numpy as np\n\n# Get predictions\ny_pred = model.predict(X_test)\n\n# Calculate metrics\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nmape = mean_absolute_percentage_error(y_test, y_pred)\n\nprint(f'MSE: {mse:.3f}')\nprint(f'RMSE: {rmse:.3f}')\nprint(f'MAE: {mae:.3f}')\nprint(f'R² Score: {r2:.3f}')\nprint(f'MAPE: {mape:.3%}')\n\n# Residual analysis\nresiduals = y_test - y_pred\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\naxes[0].scatter(y_pred, residuals, alpha=0.5)\naxes[0].axhline(y=0, color='r', linestyle='--')\naxes[0].set_xlabel('Predicted')\naxes[0].set_ylabel('Residuals')\naxes[0].set_title('Residual Plot')\n\naxes[1].hist(residuals, bins=30, edgecolor='black')\naxes[1].set_xlabel('Residuals')\naxes[1].set_title('Residual Distribution')\nplt.tight_layout()\nplt.savefig('residual_analysis.png')",
      "best_practices": [
        "RMSE penalizes large errors more than MAE",
        "Use MAE for interpretability (same unit as target)",
        "R² indicates variance explained (0-1 scale)",
        "Always check residual plots for patterns",
        "MAPE is undefined when y_test contains zeros"
      ]
    }
  },
  "model_persistence": {
    "saving_loading": {
      "description": "Save and load trained models",
      "use_when": "Deploying or sharing trained models",
      "code_example": "import joblib\nimport pickle\n\n# Method 1: joblib (recommended for sklearn)\njoblib.dump(model, 'model.joblib')\nloaded_model = joblib.load('model.joblib')\n\n# Method 2: pickle\nwith open('model.pkl', 'wb') as f:\n    pickle.dump(model, f)\n\nwith open('model.pkl', 'rb') as f:\n    loaded_model = pickle.load(f)\n\n# Save entire pipeline (recommended)\njoblib.dump(pipeline, 'pipeline.joblib')\n\n# Save with metadata\nimport json\nfrom datetime import datetime\n\nmetadata = {\n    'model_type': 'RandomForestClassifier',\n    'trained_at': datetime.now().isoformat(),\n    'features': list(X_train.columns),\n    'target': 'target_column',\n    'metrics': {\n        'accuracy': 0.95,\n        'f1': 0.93\n    },\n    'version': '1.0.0'\n}\n\nwith open('model_metadata.json', 'w') as f:\n    json.dump(metadata, f, indent=2)",
      "best_practices": [
        "Use joblib for scikit-learn models",
        "Save entire pipeline, not just model",
        "Include metadata with model",
        "Version your models",
        "Test loaded model before deployment"
      ]
    }
  },
  "experiment_tracking": {
    "manual_tracking": {
      "description": "Simple experiment tracking without external tools",
      "use_when": "Quick experiments or no MLOps infrastructure",
      "code_example": "import pandas as pd\nfrom datetime import datetime\nimport json\nimport os\n\nclass ExperimentTracker:\n    def __init__(self, experiment_name: str, log_dir: str = 'experiments'):\n        self.experiment_name = experiment_name\n        self.log_dir = log_dir\n        self.run_id = datetime.now().strftime('%Y%m%d_%H%M%S')\n        os.makedirs(f'{log_dir}/{experiment_name}', exist_ok=True)\n    \n    def log_params(self, params: dict):\n        self.params = params\n    \n    def log_metrics(self, metrics: dict):\n        self.metrics = metrics\n    \n    def log_model(self, model, model_name: str):\n        import joblib\n        path = f'{self.log_dir}/{self.experiment_name}/{self.run_id}_{model_name}.joblib'\n        joblib.dump(model, path)\n        self.model_path = path\n    \n    def save_run(self):\n        run_data = {\n            'run_id': self.run_id,\n            'experiment': self.experiment_name,\n            'timestamp': datetime.now().isoformat(),\n            'params': self.params,\n            'metrics': self.metrics,\n            'model_path': getattr(self, 'model_path', None)\n        }\n        \n        path = f'{self.log_dir}/{self.experiment_name}/{self.run_id}_run.json'\n        with open(path, 'w') as f:\n            json.dump(run_data, f, indent=2)\n        print(f'Run saved: {path}')\n\n# Usage\ntracker = ExperimentTracker('classification_v1')\ntracker.log_params({'n_estimators': 100, 'max_depth': 10})\ntracker.log_metrics({'accuracy': 0.95, 'f1': 0.93})\ntracker.log_model(model, 'random_forest')\ntracker.save_run()",
      "best_practices": [
        "Always log hyperparameters and metrics",
        "Use consistent naming conventions",
        "Include timestamp in run ID",
        "Store model artifacts with metadata",
        "Consider MLflow for production"
      ]
    }
  },
  "anti_patterns": {
    "data_leakage": {
      "description": "Using test data information during training",
      "problem": "Overly optimistic performance estimates",
      "solution": "Fit preprocessing only on training data; use pipelines"
    },
    "target_leakage": {
      "description": "Features that contain information about the target that won't be available at prediction time",
      "problem": "Model performs well in training but fails in production",
      "solution": "Carefully review feature engineering; understand data collection timeline"
    },
    "training_on_test": {
      "description": "Using test set for model selection or tuning",
      "problem": "Test set no longer provides unbiased estimate",
      "solution": "Use train/validation/test split or cross-validation"
    },
    "no_baseline": {
      "description": "Not comparing model to simple baseline",
      "problem": "Cannot assess if complex model is worthwhile",
      "solution": "Always compare to DummyClassifier/DummyRegressor"
    },
    "ignoring_class_imbalance": {
      "description": "Using accuracy on imbalanced datasets",
      "problem": "Model may just predict majority class",
      "solution": "Use F1, precision, recall, or ROC-AUC; handle imbalance"
    }
  },
  "best_practices_summary": [
    "Always split data before any preprocessing",
    "Use pipelines to prevent data leakage",
    "Set random_state for reproducibility",
    "Start with simple baselines",
    "Choose metrics aligned with business goals",
    "Document all preprocessing decisions",
    "Version your models and data",
    "Validate on held-out test set only once",
    "Monitor for data drift in production"
  ]
}
