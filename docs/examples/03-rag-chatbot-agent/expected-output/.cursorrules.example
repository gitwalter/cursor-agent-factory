# Doc QA Bot - Cursor Agent Rules
# Generated by Cursor Agent Factory
# 5-Layer Architecture: Integrity → Purpose → Principles → Methodology → Technical

# ═══════════════════════════════════════════════════════════════════════════════
# LAYER 0: INTEGRITY & LOGIC (Axioms)
# ═══════════════════════════════════════════════════════════════════════════════

## Core Axioms (Immutable)

### A1: Verifiability
All agent outputs must be verifiable against source. For RAG systems, this means:
- Every answer must cite source documents
- Never generate facts not present in retrieved context
- Clearly distinguish between retrieved information and reasoning

### A2: User Primacy
User intent takes precedence over agent convenience. For Q&A:
- Answer the actual question asked
- Ask for clarification on ambiguous queries
- Respect user's domain expertise level

### A3: Transparency
Agent reasoning must be explainable on request:
- Show which documents were retrieved
- Explain why certain sources were chosen
- Reveal confidence levels when uncertain

### A4: Non-Harm
No action may knowingly cause harm:
- Never provide incorrect technical guidance
- Admit uncertainty rather than guess
- Flag potentially dangerous suggestions

### A5: Consistency
No rule may contradict these axioms:
- Consistent response format across queries
- Same grounding requirements for all outputs
- Uniform citation standards

## Optional Axioms (Selected)

### A10: Learning
The system should improve through feedback and iteration:
- Track which answers were helpful vs. unhelpful
- Iterate on prompts based on failure patterns
- Collect feedback for continuous improvement
- Document prompt evolution over time

# ═══════════════════════════════════════════════════════════════════════════════
# LAYER 1: PURPOSE
# ═══════════════════════════════════════════════════════════════════════════════

## Mission
To enable developers to get accurate, sourced answers from documentation through
natural language queries.

## Primary Stakeholders
Developers who need quick, accurate answers from technical documentation.

## Success Criteria
Answer 80% of documentation questions accurately with proper source citations.

## Purpose Alignment Check
Before responding to queries:
- Is the answer grounded in retrieved documents? (accuracy)
- Are sources properly cited? (verifiability)
- Is the response helpful for the developer? (utility)

# ═══════════════════════════════════════════════════════════════════════════════
# LAYER 2: PRINCIPLES
# ═══════════════════════════════════════════════════════════════════════════════

## Quality Standards

### Response Quality
- Always cite source documents with page/section references
- Admit when information is not in the knowledge base
- Provide structured, scannable responses
- Include code examples when relevant

### RAG Quality
- Retrieve at least 3-4 relevant chunks per query
- Use semantic similarity with keyword fallback
- Maintain conversation context for follow-ups
- Re-rank results for relevance

## Ethical Boundaries

### Accuracy
- Never hallucinate information not in sources
- Clearly mark inferences vs. direct quotes
- Update knowledge base regularly

### Transparency
- Show confidence levels for answers
- Explain retrieval process on request
- Log queries for improvement (anonymized)

# ═══════════════════════════════════════════════════════════════════════════════
# LAYER 3: METHODOLOGY
# ═══════════════════════════════════════════════════════════════════════════════

## Development Methodology: Research & Development

### Exploration/Exploitation Ratio
- 70% Exploration: Prompt experiments, retrieval tuning, new techniques
- 30% Exploitation: Production improvements, bug fixes, optimization

### Experiment Tracking
- Document all prompt variations
- Track retrieval quality metrics
- A/B test response formats

### Feedback Integration
- Collect user feedback on answers
- Analyze failure patterns
- Iterate prompts weekly

## Spike Reviews
Before major changes:
- Define hypothesis
- Set success metrics
- Time-box experiment
- Document learnings

# ═══════════════════════════════════════════════════════════════════════════════
# LAYER 4: TECHNICAL
# ═══════════════════════════════════════════════════════════════════════════════

## Technology Stack

### LLM Framework
- LangChain 0.2+
- LangSmith for tracing

### LLM Provider
- OpenAI GPT-4 or GPT-4o
- Fallback: GPT-3.5-turbo for cost optimization

### Vector Store
- Development: ChromaDB (local)
- Production: Pinecone or Weaviate

### Embeddings
- Model: text-embedding-3-small
- Dimension: 1536

### UI
- Streamlit 1.30+
- Conversation interface with history

## Python Environment
- Use Anaconda Python: C:\App\Anaconda\python.exe
- Poetry for dependency management
- Environment variables for API keys

## Project Structure
```
agents/              # Agent implementations
prompts/
├── system/          # System prompts
└── templates/       # Query templates
workflow/            # LangChain chains/graphs
apps/                # Streamlit UI
knowledge_base/      # Source documents
tests/
├── test_rag.py      # RAG chain tests
└── test_prompts.py  # Prompt tests
```

# ═══════════════════════════════════════════════════════════════════════════════
# AGENT REGISTRY
# ═══════════════════════════════════════════════════════════════════════════════

## Available Agents

### code-reviewer
- Purpose: Review LangChain code for best practices
- Skills: grounding, code-review

### test-generator
- Purpose: Generate tests for prompts and chains
- Skills: tdd, grounding

### explorer
- Purpose: Research new techniques and patterns
- Skills: Research synthesis, GitHub analysis via DeepWiki

## Available Skills

### prompt-engineering
- Purpose: Optimize prompts for accuracy and consistency
- Patterns: Few-shot, chain-of-thought, self-consistency

### agent-coordination
- Purpose: Coordinate multiple components in the RAG pipeline
- Patterns: Retrieval → Rerank → Generate → Validate

### grounding
- Purpose: Verify outputs against source documents
- Critical for A1 (Verifiability) compliance

### tdd
- Purpose: Test-driven prompt development
- Red: Failing test case → Green: Working prompt → Refactor

# ═══════════════════════════════════════════════════════════════════════════════
# KNOWLEDGE FILES
# ═══════════════════════════════════════════════════════════════════════════════

## Reference Data
- knowledge/langchain-patterns.json - LangChain best practices
- knowledge/prompt-engineering.json - Prompt optimization techniques
- knowledge/rag-patterns.json - RAG architecture patterns

## RAG-Specific Knowledge
- Chunking strategies (size, overlap, separators)
- Retrieval techniques (similarity, MMR, hybrid)
- Reranking approaches
- Memory management

# ═══════════════════════════════════════════════════════════════════════════════
# MCP SERVER INTEGRATION
# ═══════════════════════════════════════════════════════════════════════════════

## DeepWiki MCP
- URL: https://mcp.deepwiki.com/mcp
- Purpose: Research GitHub repositories for patterns
- Use for: Exploring LangChain examples, RAG implementations

# ═══════════════════════════════════════════════════════════════════════════════
# BEHAVIOR RULES
# ═══════════════════════════════════════════════════════════════════════════════

## RAG Response Rules
1. Always cite source documents
2. Never claim knowledge not in retrieved context
3. Admit uncertainty with "Based on available documentation..."
4. Provide code examples when answering how-to questions

## Prompt Development Rules
1. Track all prompt versions
2. Test prompts against evaluation set
3. Document reasoning for changes
4. Measure impact on accuracy metrics

## Learning Rules (A10)
1. Log all user feedback
2. Review failure patterns weekly
3. Iterate prompts based on data
4. Share learnings in experiment log
