{
  "metadata": {
    "patternId": "prompt-engineering",
    "patternName": "Prompt Engineering Skill",
    "category": "ai-development",
    "stackAgnostic": true,
    "description": "Systematic approach to designing, testing, and optimizing prompts for LLM-powered applications",
    "composable": true,
    "version": "1.0.0"
  },
  "frontmatter": {
    "name": "prompt-engineering",
    "description": "Design, test, and optimize prompts for LLM-powered agents and applications",
    "type": "skill",
    "knowledge": ["prompt-engineering.json", "langchain-patterns.json"],
    "templates": ["templates/ai/prompt/"]
  },
  "sections": {
    "title": "Prompt Engineering Skill",
    "introduction": "Guides the systematic design, testing, and optimization of prompts for LLM-powered applications. Covers techniques from basic instruction writing to advanced chain-of-thought and structured output patterns.",
    "whenToUse": [
      "When creating new agent prompts or system instructions",
      "When improving agent response quality or consistency",
      "When debugging unexpected agent behaviors",
      "When optimizing prompts for cost, latency, or accuracy",
      "When implementing structured outputs with Pydantic models"
    ],
    "process": [
      {
        "step": 1,
        "name": "Define Objective",
        "description": "Clearly articulate what the prompt should achieve",
        "actions": [
          "Identify the specific task or behavior needed",
          "Define success criteria and edge cases",
          "List required inputs and expected outputs",
          "Identify constraints (format, length, tone)"
        ],
        "outputs": ["Prompt specification document"]
      },
      {
        "step": 2,
        "name": "Choose Technique",
        "description": "Select appropriate prompting techniques based on task complexity",
        "techniques": {
          "zero-shot": "Simple tasks with clear instructions, no examples needed",
          "few-shot": "Tasks needing examples to demonstrate format or style",
          "chain-of-thought": "Complex reasoning requiring step-by-step thinking",
          "role-prompting": "Tasks benefiting from specific persona or expertise",
          "structured-output": "Tasks requiring consistent JSON/structured responses"
        },
        "actions": [
          "Assess task complexity and reasoning requirements",
          "Evaluate need for examples vs. instructions",
          "Consider output format requirements"
        ]
      },
      {
        "step": 3,
        "name": "Draft Prompt",
        "description": "Create initial prompt following best practices",
        "structure": {
          "system": "Role, capabilities, constraints, and global instructions",
          "context": "Background information and relevant knowledge",
          "task": "Specific instruction for the current request",
          "format": "Expected output structure and format",
          "examples": "Few-shot examples if using that technique"
        },
        "actions": [
          "Write clear, unambiguous instructions",
          "Include explicit format specifications",
          "Add constraints and guardrails",
          "Include examples if using few-shot"
        ]
      },
      {
        "step": 4,
        "name": "Test and Iterate",
        "description": "Systematically test prompt with varied inputs",
        "actions": [
          "Test with happy path scenarios",
          "Test with edge cases and adversarial inputs",
          "Measure response consistency across runs",
          "Compare against success criteria",
          "Document failure modes"
        ],
        "outputs": ["Test results", "Failure analysis"]
      },
      {
        "step": 5,
        "name": "Optimize",
        "description": "Refine prompt based on test results",
        "optimizations": {
          "clarity": "Remove ambiguity, add specific examples",
          "consistency": "Add format constraints, use structured outputs",
          "cost": "Reduce token count while maintaining quality",
          "latency": "Simplify instructions, reduce few-shot examples",
          "accuracy": "Add chain-of-thought, more examples"
        },
        "actions": [
          "Address identified failure modes",
          "Reduce unnecessary verbosity",
          "Add guardrails for edge cases"
        ]
      },
      {
        "step": 6,
        "name": "Document and Version",
        "description": "Document final prompt with rationale",
        "actions": [
          "Document prompt purpose and context",
          "Record design decisions and trade-offs",
          "Version control prompt changes",
          "Add to prompt library if reusable"
        ],
        "outputs": ["Documented prompt", "Version history"]
      }
    ],
    "techniques": {
      "chain-of-thought": {
        "description": "Ask model to show reasoning steps before final answer",
        "pattern": "Think step by step. First, [analysis]. Then, [reasoning]. Finally, [conclusion].",
        "useCase": "Complex reasoning, math, multi-step problems"
      },
      "few-shot-learning": {
        "description": "Provide examples demonstrating desired behavior",
        "pattern": "Here are examples:\n\nInput: X\nOutput: Y\n\nInput: A\nOutput: B\n\nNow process:\nInput: [user_input]",
        "useCase": "Format consistency, style matching, classification"
      },
      "structured-outputs": {
        "description": "Use Pydantic models or JSON schema for consistent output",
        "pattern": "Respond with valid JSON matching this schema: {schema}",
        "useCase": "API responses, data extraction, structured data"
      },
      "role-prompting": {
        "description": "Assign specific role or persona to the model",
        "pattern": "You are an expert [role] with [years] of experience in [domain].",
        "useCase": "Domain expertise, consistent persona, specialized knowledge"
      },
      "self-consistency": {
        "description": "Generate multiple responses and select most common",
        "pattern": "Generate 3 independent solutions, then select the most consistent answer.",
        "useCase": "High-stakes decisions, reducing hallucination"
      }
    },
    "antiPatterns": [
      {"pattern": "Vague instructions", "problem": "Inconsistent outputs", "fix": "Be specific about format, length, and style"},
      {"pattern": "Overly long prompts", "problem": "Higher cost and latency", "fix": "Distill to essential instructions"},
      {"pattern": "No examples for complex tasks", "problem": "Incorrect output format", "fix": "Add few-shot examples"},
      {"pattern": "Ignoring edge cases", "problem": "Failures in production", "fix": "Test and add guardrails"},
      {"pattern": "Hardcoded context", "problem": "Inflexible prompts", "fix": "Use template variables"}
    ],
    "importantRules": [
      "Start simple - add complexity only when needed",
      "Test with diverse inputs including edge cases",
      "Document design decisions and trade-offs",
      "Version control prompts like code",
      "Measure before and after optimizations",
      "Consider cost and latency alongside accuracy"
    ],
    "fallbackProcedures": [
      {"condition": "If output format is inconsistent", "action": "Add structured output with Pydantic model"},
      {"condition": "If reasoning is flawed", "action": "Add chain-of-thought prompting"},
      {"condition": "If model lacks domain knowledge", "action": "Add relevant context or use RAG"},
      {"condition": "If responses are too verbose", "action": "Add explicit length constraints"}
    ],
    "references": [
      "knowledge/prompt-engineering.json",
      "knowledge/langchain-patterns.json",
      "templates/ai/prompt/"
    ]
  },
  "variables": [
    {"name": "{TASK_DESCRIPTION}", "description": "What the prompt should accomplish", "required": true},
    {"name": "{OUTPUT_FORMAT}", "description": "Expected response format", "required": true},
    {"name": "{EXAMPLES}", "description": "Few-shot examples if applicable", "required": false}
  ]
}
